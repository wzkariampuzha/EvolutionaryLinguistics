{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and calculating lexicon size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "The purpose of this investigation is to define the size of the lexicon over any arbitrary time interval. What is language? What is a lexicon?  \n",
    "<br></br>\n",
    "The *a priori* assumption is that a language exists in an open set of time(s) with members (words, morphemes, lexemes, grammatical structures, idioms, etc.) who exist at at least one time step *t* within that set. However, I posit that a lexicon can only be defined in a closed set of time(s).  \n",
    "<br></br>\n",
    "I define the lexicon to be the subset of language whose members (lexemes) must be true for all time steps *t* in T = {t for t in range(t<sub>i</sub>,t<sub>f</sub>)}; where t<sub>i</sub> is initial time (lower bound of variable interval), t<sub>f</sub> is final time (upper bound of variable interval), and the size of time step *t* is a an arbitrary distinction that can be defined by the data, as time is likely a continuous variable.  \n",
    "<br></br>\n",
    "Hence the size of the lexicon will be the cardinality of the lexicon set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals:\n",
    "1. Load the \\*\\-COMPLETE.json files\n",
    "2. Form dictionary of form  \n",
    "        {unigram: {frequency: sum_usage/total_usage of all lexemes in time interval,\n",
    "                   sum_usage: sum total of lexeme counts across time interval,\n",
    "                   median_usage: median lexeme counts over time interval,\n",
    "                   mean_usage: average lexeme counts per year over time interval,\n",
    "                   max_usage: maximum usage of lexeme at single year in time interval,\n",
    "                   min_usage: minimum usage of lexeme at single year in time interval}\n",
    "            ...}  \n",
    "3. Concatenate the dictionaries\n",
    "4. Save as a single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import statistics\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(directory,file_path):\n",
    "    with open(directory+file_path,'r') as f:\n",
    "        json_dictionary = json.load(f)\n",
    "        f.close()\n",
    "    return json_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(dictionary,directory,file_name):\n",
    "    output = file_name[:-3]\n",
    "    if len(dictionary)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            json.dump(dictionary, f_out)\n",
    "        print('SAVED: ',output,len(dictionary))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ngrams, t_start, t_end):\n",
    "    years = [str(i) for i in range(t_start,t_end+1)]\n",
    "    unigram_dict = dict()\n",
    "    for word in tqdm(ngrams.keys()):\n",
    "        match_count_by_year = []\n",
    "        for year in years:\n",
    "            if year in ngrams[word].keys():\n",
    "                match_count_by_year.append(ngrams[word][year])\n",
    "            else:\n",
    "                #Zeroes are necessary for smoothing\n",
    "                match_count_by_year.append(0)\n",
    "        unigram_dict[word] = match_count_by_year\n",
    "    return unigram_dict, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(unigram_dict, years, smoothing = 5):\n",
    "    df = pd.DataFrame.from_dict(unigram_dict #take in the dictionary\n",
    "                    ).rolling(smoothing,center=True #create frames of size 5 (smoothing value), and replace value in middle\n",
    "                    ).mean( #average accross those frames\n",
    "                    ).rename({i:years[i] for i in range(len(years))}, axis = 'index' #rename the indices to years\n",
    "                    ).dropna()\n",
    "    \n",
    "    years = list(df.index.values)\n",
    "    ngrams = df.to_dict(orient = 'list')\n",
    "    return ngrams, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_lexicon(ngrams):\n",
    "    lexicon = dict()\n",
    "    total_usage = 0\n",
    "    for unigram in tqdm(ngrams.keys()):\n",
    "        frequency_list = ngrams[unigram]\n",
    "        #If there are no zeroes in the list of frequencies for that unigram\n",
    "        if 0 not in frequency_list:\n",
    "            sum_usage = sum(frequency_list)\n",
    "            median_usage = statistics.median(frequency_list)\n",
    "            mean_usage = statistics.mean(frequency_list)\n",
    "            max_usage = max(frequency_list)\n",
    "            min_usage = min(frequency_list)\n",
    "            \n",
    "            total_usage+=sum_usage\n",
    "            lexicon[unigram] = {'sum_usage':sum_usage,\n",
    "                                'median_usage':median_usage,\n",
    "                                'mean_usage':mean_usage,\n",
    "                                'max_usage':max_usage,\n",
    "                                'min_usage':min_usage}\n",
    "        \n",
    "    return lexicon, total_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency(lexicon,total_usage):\n",
    "    for lexeme in tqdm(lexicon.keys()):\n",
    "        lexicon[lexeme]['frequency'] = lexicon[lexeme]['sum_usage']/total_usage\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directory, t_start, t_end, t_step):\n",
    "    final_dict = dict()\n",
    "    files = os.listdir(os.path.abspath(directory))\n",
    "    for file_name in files:\n",
    "        if '-COMPLETE.json.gz' in file_name:\n",
    "            print(file_path)\n",
    "            ngrams = open_gzipped_json(directory,file_name)\n",
    "            print('Opened json')\n",
    "            unigram_dict, years = normalize(ngrams, t_start, t_end)\n",
    "            del ngrams\n",
    "            print('Normalized')\n",
    "            ngrams, years = smoothing(unigram_dict, years, t_step)\n",
    "            del unigram_dict\n",
    "            print('Smoothed')\n",
    "            lexicon, total_usage = return_lexicon(ngrams)\n",
    "            del ngrams\n",
    "            print('Got lexicon')\n",
    "            lexicon = add_frequency(lexicon, total_usage)\n",
    "        \n",
    "            final_dict.update(lexicon)\n",
    "            \n",
    "    save_json(final_dict,directory,'LEXICON_YEAR_'+str(years[0])+'-'+str(years[-1])+'_STEP_'+t_step)\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_start = 1995 #Input the year that you want to consider as the lower bound of the lexicon\n",
    "t_end = 2018 #Input the year that you want to consider as the upper bound of the lexicon \n",
    "t_step = 3 #Smoothing is a more advanced way to increase the time step (and is code reuse). \n",
    "directory = 'C:\\\\Users\\\\wzkar\\\\Documents\\\\Linguistic Research\\\\Ngrams\\\\unigram_data\\\\'\n",
    "if t_start>=t_end:\n",
    "    raise ValueError('Re-Input start year and end year')\n",
    "else:\n",
    "    t_interval = t_end-t_start\n",
    "    print('Range of time for calculating lexicon size is', t_interval-(t_step-1),'years')\n",
    "    lexicon = main(directory, t_start, t_end, t_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of Lexicon is ',len(lexicon.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_counts(dictionary,count_type,num_hits=25,head = True):\n",
    "    return OrderedDict(sorted(dictionary.items(), key=lambda x: x[1][count_type], reverse=head)[:num_hits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequency = top_counts(lexicon,'frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_median = top_counts(lexicon,'median_usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_max = top_counts(lexicon,'max_usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evol-ling]",
   "language": "python",
   "name": "conda-env-evol-ling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
