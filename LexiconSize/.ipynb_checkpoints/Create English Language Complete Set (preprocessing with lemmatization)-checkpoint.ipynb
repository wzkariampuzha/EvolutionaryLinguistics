{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Ngrams Analysis\n",
    "## An Evolutionary Investigation\n",
    "\n",
    "The purpose of this is to filter through the entire dataset without limiting years. It will create \\*\\-COMPLETE.json files. It can be used to find the size of the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original Ngrams analysis](https://github.com/Aaronasnx/Google-preprocessing/blob/main/ngram%20project.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "#for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#For checking if the word has any non-English-alphabetical letters\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install Unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "import re\n",
    "#For the Google POS tagging mapping\n",
    "underscore = re.compile('_{1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NLTK POS Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "            `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "            for [satellite adjectives](https://stackoverflow.com/questions/18817396/what-part-of-speech-does-s-stand-for-in-wordnet-synsets).  \n",
    "  \n",
    "  Syntax:\n",
    "`lemmatizer.lemmatize(word)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google Tags](https://books.google.com/ngrams/info)\n",
    "These tags can either stand alone (\\_PRON\\_) or can be appended to a word (she_PRON)\n",
    "- _NOUN_\t\t\n",
    "- _VERB_\t\n",
    "- _ADJ_\tadjective\n",
    "- _ADV_\tadverb\n",
    "- _PRON_\tpronoun\n",
    "- _DET_\tdeterminer or article\n",
    "- _ADP_\tan adposition: either a preposition or a postposition\n",
    "- _NUM_\tnumeral\n",
    "- _CONJ_\tconjunction\n",
    "- _PRT_\tparticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sets which are going to be used in the unigram tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation).union({'“','”'})\n",
    "#ALPHABET = set(string.ascii_letters)\n",
    "\n",
    "DIGITS = set(string.digits)\n",
    "VOWELS = set(\"aeiouyAEIOUY\")\n",
    "#Excluding '_' (underscore) from DASHES precludes the tagged 1grams \"_NOUN\", add it to also include the tagged 1grams\n",
    "DASHES = {'—','–','—','―','‒','-','_'}\n",
    "PUNCTUATION.difference_update(DASHES)\n",
    "STOPS = PUNCTUATION.union(DIGITS)\n",
    "#GOOGLE_TAGS = {'_NOUN','_VERB','_ADJ','_ADV','_PRON','_DET','_ADP','_NUM','_CONJ','_PRT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dias'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Demo of unidecode to show how will use it to filter out accents and non-English letters\n",
    "unidecode('días', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "unigram = 'kožušček'\n",
    "test = unidecode(unigram, errors='replace')\n",
    "if test == unigram:\n",
    "    print('yes')\n",
    "    pass\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to open Gzip files](https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_gzip(directory,file_path):\n",
    "    with gzip.open(directory+file_path,'r') as f_in:\n",
    "        rows = [x.decode('utf8').strip() for x in f_in.readlines()]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(ngram_dict,directory,file_path):\n",
    "    output = file_path[:-3]+'-COMPLETE.json'\n",
    "    if len(ngram_dict)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            json.dump(ngram_dict, f_out)\n",
    "        print('SAVED: ',output,len(ngram_dict))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2tuple(string):\n",
    "    year,match_count,volume_count = tuple(string.split(','))\n",
    "    return int(year),int(match_count),int(volume_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tests(unigram):\n",
    "    #Exclude words with more than one underscore, can make this != to only include tagged words\n",
    "    if len(underscore.findall(unigram))!=1:\n",
    "        return False\n",
    "    \n",
    "    #Checks each character in the unigram against the characters in the STOP set. (character level filtering) - no punctuation or digits allowed\n",
    "    if set(unigram).intersection(STOPS):\n",
    "        return False\n",
    "    \n",
    "    #Excluded all of the form _PRON_ (or anything that starts or ends with an underscore)\n",
    "    if unigram[0] == '_' or unigram[-1] == '_':\n",
    "        return False\n",
    "    \n",
    "    #must have a vowel (presupposes that it must also have a letter of the alphabet inside)\n",
    "    if not set(unigram).intersection(VOWELS):\n",
    "        return False \n",
    "    \n",
    "    #Words cannot start or end with dashes\n",
    "    if unigram[0] in DASHES or unigram[-1] in DASHES:\n",
    "        return False\n",
    "    \n",
    "    #must have 0 non-english letters\n",
    "    test = unidecode(unigram, errors='replace')\n",
    "    if test != unigram:\n",
    "        return False\n",
    "    \n",
    "    #Can implement more tests here if you need to do more filtering\n",
    "    \n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps Google pos_tag to Wordnet pos_tag\n",
    "def POS_mapper(pos_tag):\n",
    "    if pos_tag == 'NOUN':\n",
    "        return \"n\"\n",
    "    if pos_tag == 'VERB':\n",
    "        return \"v\"\n",
    "    if pos_tag == 'ADJ':\n",
    "        return \"a\"\n",
    "    if pos_tag == 'ADV':\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"n\" #Default for wordnet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ngrams(directory,file_path):\n",
    "    \n",
    "    #rows = open_gzip(directory,file_path)\n",
    "    ngram_dict = dict()\n",
    "\n",
    "    #This implementation uses {1gram:{year:match_count ...} ...}\n",
    "    i=0\n",
    "    for row in tqdm(open_gzip(directory,file_path)):\n",
    "        columns = row.split('\\t')\n",
    "        #unigram is the first entry, the rest of the entries are of the form year,match_count,volume_count\\t n times, where n is variable each line\n",
    "        \n",
    "        unigram = columns[0]\n",
    "        #If it passes the word tests continue parsing and lemmatizing the unigram\n",
    "        if unigram_tests(unigram):\n",
    "            pos = \"n\" #Default for wordnet lemmatizer\n",
    "            word_tag = underscore.split(unigram) # list of [word,tag]\n",
    "            \n",
    "            #maps Google tag to Wordnet tag\n",
    "            pos = POS_mapper(word_tag[1])\n",
    "            \n",
    "            #Removes the tag before processing unigram string\n",
    "            unigram = word_tag[0]\n",
    "            \n",
    "            #Lemmatize based on POS\n",
    "            unigram = lemmatizer.lemmatize(unigram.lower().strip(),pos)\n",
    "            \n",
    "            #Adds the tag back onto the unigram\n",
    "            unigram+='_'+word_tag[1]\n",
    "            \n",
    "            #Parse the new entry and create a dictionary of records in form {year:match_count}\n",
    "            records = dict()\n",
    "            for entry in columns[1:]:\n",
    "                year,match_count,volume_count = csv2tuple(str(entry))\n",
    "                #This is the crucial filtering by volume count because only words in >1 volume are reasonably assumed to be used by >1 person\n",
    "                #Words only used by one person - which translates the computational parameter 1 volume - are not considered part of the lexicon\n",
    "                if volume_count>1:\n",
    "                    records[year] = match_count\n",
    "\n",
    "            #Modify the dictionary if new entry is already there, else just add it as a new unigram:records to the dict\n",
    "            if unigram in ngram_dict.keys():\n",
    "                #accessing the ngram dictionary and seeing if each year is present, if so add match count, else add a new record entry to the dictionary.\n",
    "                for yr, match_ct in records.items(): #each record should be of the form {year:match_count}\n",
    "                    #If the year in the new record is in the dict for this 1gram, then find where it is.\n",
    "                    if yr in ngram_dict[unigram].keys():\n",
    "                        ngram_dict[unigram][yr] += match_ct\n",
    "                    else:\n",
    "                        #This just adds the record to the end, will need to sort later\n",
    "                        ngram_dict[unigram][yr] = match_ct\n",
    "            else:\n",
    "                ngram_dict[unigram] = records\n",
    "    #Save as JSON\n",
    "    save_json(ngram_dict,directory,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2396510/2396510 [00:16<00:00, 142907.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00000-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3345476/3345476 [00:31<00:00, 106383.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00001-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3315859/3315859 [00:27<00:00, 119869.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00002-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3081673/3081673 [00:24<00:00, 126644.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00003-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3377697/3377697 [00:27<00:00, 123188.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00004-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3375570/3375570 [00:26<00:00, 129096.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram dict empty 1-00005-of-00024-COMPLETE.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3103866/3103866 [02:24<00:00, 21471.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00006-of-00024-COMPLETE.json 582617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3384843/3384843 [03:12<00:00, 17560.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00007-of-00024-COMPLETE.json 1162149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3408143/3408143 [04:28<00:00, 12692.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00008-of-00024-COMPLETE.json 1227039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3109631/3109631 [04:16<00:00, 12125.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00009-of-00024-COMPLETE.json 1081183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3365531/3365531 [05:36<00:00, 9990.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00010-of-00024-COMPLETE.json 1192753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3386487/3386487 [04:32<00:00, 12416.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00011-of-00024-COMPLETE.json 1205150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135145/3135145 [04:15<00:00, 12257.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00012-of-00024-COMPLETE.json 1082701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3402459/3402459 [04:35<00:00, 12368.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  1-00013-of-00024-COMPLETE.json 1214351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392643/3392643 [03:32<00:00, 16001.26it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory = 'C:\\\\Users\\\\wzkar\\\\Documents\\\\Linguistic Research\\\\Ngrams\\\\unigram_data\\\\'\n",
    "files = os.listdir(os.path.abspath(directory))\n",
    "for file_path in files:\n",
    "    if '.gz' in file_path and not '.json' in file_path:\n",
    "        preprocess_ngrams(directory,file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evol-ling]",
   "language": "python",
   "name": "conda-env-evol-ling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
