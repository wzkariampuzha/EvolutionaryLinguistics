{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Investigate birth and death among closed classes of words\n",
    "1. Load the \\*\\_CLOSED_CLASSES.json files\n",
    "2. Separate the word from the part of speech and form JSON of form  \n",
    "        {unigram: {pos:'pos',\n",
    "                   max: max_usage,\n",
    "                   median_all: median_all,\n",
    "                   median_in_use:median_in_use,\n",
    "                   mean_all: mean_all,\n",
    "                   mean_in_use:mean_in_use,\n",
    "                   birth_years: [year1, year2, ...],\n",
    "                   death_years: [year1, year2, ...]}\n",
    "            ...}\n",
    "    Where `me(di)an_all` is the me(di)an of the frequencies of usage at all points in the time interval.\n",
    "    and `me(di)an_in_use` is the me(di)an of the frequencies of usage only when actually in use (frequency >0)  \n",
    "    \n",
    "3. Concatenate the final dictionaries\n",
    "4. Save as a single JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available parts of speech:\n",
    "- _PRON_\tpronoun\n",
    "- _DET_\tdeterminer or article\n",
    "- _ADP_\tan adposition: either a preposition or a postposition\n",
    "- _CONJ_\tconjunction\n",
    "- _PRT_\tparticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the \\*\\_CLOSED_CLASSES.json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "#For the Google POS tagging\n",
    "underscore = re.compile('_{1}')\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(directory,file_path):\n",
    "    with open(directory+file_path,'r') as f:\n",
    "        ngrams = json.load(f)\n",
    "        f.close()\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ngrams):\n",
    "    years = [str(i) for i in range(1800,2020)]\n",
    "    unigram_dict = dict()\n",
    "    for word in tqdm(ngrams.keys()):\n",
    "        match_count_by_year = []\n",
    "        for year in years:\n",
    "            if year in ngrams[word].keys():\n",
    "                match_count_by_year.append(ngrams[word][year])\n",
    "            else:\n",
    "                #Zeroes are necessary for smoothing\n",
    "                match_count_by_year.append(0)\n",
    "        unigram_dict[word] = match_count_by_year\n",
    "    return unigram_dict, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(unigram_dict, years, smoothing = 5):\n",
    "    df = pd.DataFrame.from_dict(unigram_dict #take in the dictionary\n",
    "                    ).rolling(smoothing,center=True #create frames of size 5 (smoothing value), and replace value in middle\n",
    "                    ).mean( #average accross those frames\n",
    "                    ).rename({i:years[i] for i in range(len(years))}, axis = 'index' #rename the indices to years\n",
    "                    ).dropna()\n",
    "\n",
    "    years_map = {i:int(year) for i, year in enumerate(df.index)}\n",
    "    ngrams = df.to_dict(orient = 'list')\n",
    "    return ngrams, years_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_birth_and_death(ngrams,years_map):\n",
    "    ngrams_analyzed = {}\n",
    "    \n",
    "    for unigram in tqdm(ngrams.keys()):\n",
    "        frequency_list = ngrams[unigram]\n",
    "        frequency_in_use_list = [f for f in frequency_list if f>0]\n",
    "        if frequency_in_use_list: #only proceed if there is some value that is greater than 0 in the frequency list\n",
    "            max_usage = max(frequency_list)\n",
    "            median_all = statistics.median(frequency_list)\n",
    "            median_in_use = statistics.median(frequency_in_use_list)\n",
    "            mean_all = statistics.mean(frequency_list)\n",
    "            mean_in_use = statistics.mean(frequency_in_use_list)\n",
    "\n",
    "            birth_years, death_years = [],[]\n",
    "            for i in range(len(frequency_list)-1):\n",
    "                #Birth\n",
    "                if frequency_list[i]==0 and frequency_list[i+1]!=0:\n",
    "                    birth_years.append(years_map[i+1])\n",
    "                #Death\n",
    "                if frequency_list[i]!=0 and frequency_list[i+1]==0:\n",
    "                    death_years.append(years_map[i])\n",
    "                #Disregarding death in the final year\n",
    "\n",
    "            if len(birth_years)+len(death_years)>0:\n",
    "                #Replace the tagged unigram with the word and place POS separately\n",
    "                word_pos = underscore.split(unigram)\n",
    "                ngrams_analyzed[word_pos[0]] = {'POS':word_pos[1],\n",
    "                                                'max_usage':max_usage,\n",
    "                                                'median_all':median_all,\n",
    "                                                'median_in_use':median_in_use,\n",
    "                                                'mean_all':mean_all,\n",
    "                                                'mean_in_use':mean_in_use,\n",
    "                                                'birth_years':birth_years,\n",
    "                                                'death_years':death_years}\n",
    "        else:\n",
    "            pass\n",
    "            #print(unigram,'had no instances of usage after smoothing.')\n",
    "            \n",
    "    return ngrams_analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(dictionary,directory,file_path):\n",
    "    output = file_path+'.json'\n",
    "    if len(dictionary)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            json.dump(dictionary, f_out)\n",
    "        print('SAVED: ',output,len(dictionary))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directory):\n",
    "    final_dict = {}\n",
    "    \n",
    "    directory = './Ngrams/'\n",
    "    files = os.listdir(directory)\n",
    "    for file_path in files:\n",
    "        if '_CLOSED_CLASSES.json' in file_path:\n",
    "            ngrams = open_json(directory,file_path)\n",
    "            unigram_dict, years = normalize(ngrams)\n",
    "            del ngrams\n",
    "            ngrams, years_map = smoothing(unigram_dict, years)\n",
    "            del unigram_dict\n",
    "            del years\n",
    "            ngrams_analyzed = analyze_birth_and_death(ngrams,years_map)\n",
    "            del ngrams\n",
    "            del years_map\n",
    "            final_dict.update(ngrams_analyzed)\n",
    "    save_json(final_dict,directory,'CLOSED_CLASSES_SORTABLE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33454/33454 [00:01<00:00, 24039.59it/s]\n",
      "100%|██████████| 33454/33454 [00:10<00:00, 3129.65it/s]\n",
      "100%|██████████| 2891/2891 [00:00<00:00, 32063.73it/s]\n",
      "100%|██████████| 2891/2891 [00:00<00:00, 4131.73it/s]\n",
      "100%|██████████| 16626/16626 [00:00<00:00, 28058.11it/s]\n",
      "100%|██████████| 16626/16626 [00:04<00:00, 4135.79it/s]\n",
      "100%|██████████| 3739/3739 [00:00<00:00, 30881.16it/s]\n",
      "100%|██████████| 3739/3739 [00:00<00:00, 4594.98it/s]\n",
      "100%|██████████| 41723/41723 [00:01<00:00, 30132.29it/s]\n",
      "100%|██████████| 41723/41723 [00:09<00:00, 4363.87it/s]\n",
      "100%|██████████| 12317/12317 [00:00<00:00, 31664.29it/s]\n",
      "100%|██████████| 12317/12317 [00:02<00:00, 4204.11it/s]\n",
      "100%|██████████| 36568/36568 [00:01<00:00, 31254.78it/s]\n",
      "100%|██████████| 36568/36568 [00:08<00:00, 4105.91it/s]\n",
      "100%|██████████| 123093/123093 [00:04<00:00, 27879.35it/s]\n",
      "100%|██████████| 123093/123093 [00:28<00:00, 4295.92it/s]\n",
      "100%|██████████| 13055/13055 [00:00<00:00, 30922.10it/s]\n",
      "100%|██████████| 13055/13055 [00:02<00:00, 4494.92it/s]\n",
      "100%|██████████| 15740/15740 [00:00<00:00, 31453.51it/s]\n",
      "100%|██████████| 15740/15740 [00:03<00:00, 4512.40it/s]\n",
      "100%|██████████| 20180/20180 [00:00<00:00, 30515.15it/s]\n",
      "100%|██████████| 20180/20180 [00:04<00:00, 4485.05it/s]\n",
      "100%|██████████| 72093/72093 [00:02<00:00, 29828.17it/s]\n",
      "100%|██████████| 72093/72093 [00:16<00:00, 4462.60it/s]\n",
      "100%|██████████| 4995/4995 [00:00<00:00, 30691.42it/s]\n",
      "100%|██████████| 4995/4995 [00:01<00:00, 4575.92it/s]\n",
      "100%|██████████| 77955/77955 [00:02<00:00, 30656.78it/s]\n",
      "100%|██████████| 77955/77955 [00:17<00:00, 4369.72it/s]\n",
      "100%|██████████| 12206/12206 [00:00<00:00, 31786.20it/s]\n",
      "100%|██████████| 12206/12206 [00:02<00:00, 4619.90it/s]\n",
      "100%|██████████| 9092/9092 [00:00<00:00, 30876.53it/s]\n",
      "100%|██████████| 9092/9092 [00:02<00:00, 4476.29it/s]\n",
      "100%|██████████| 4368/4368 [00:00<00:00, 31687.37it/s]\n",
      "100%|██████████| 4368/4368 [00:00<00:00, 4545.18it/s]\n",
      "100%|██████████| 122872/122872 [00:04<00:00, 30569.52it/s]\n",
      "100%|██████████| 122872/122872 [00:28<00:00, 4368.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED:  CLOSED_CLASSES_SORTABLE.json 440403\n",
      "CPU times: user 4min 34s, sys: 5.38 s, total: 4min 40s\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main(directory = './Ngrams/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
