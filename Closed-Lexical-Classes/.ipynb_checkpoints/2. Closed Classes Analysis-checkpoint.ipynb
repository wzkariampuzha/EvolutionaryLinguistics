{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Investigate birth and death among closed classes of words\n",
    "1. Load the \\*\\_CLOSED_CLASSES.json files\n",
    "2. Separate the word from the part of speech and form JSON of form  \n",
    "        {unigram: {pos:'pos',\n",
    "                   max: max_usage,\n",
    "                   median_all: median_all,\n",
    "                   median_in_use:median_in_use,\n",
    "                   mean_all: mean_all,\n",
    "                   mean_in_use:mean_in_use,\n",
    "                   birth_years: [year1, year2, ...],\n",
    "                   death_years: [year1, year2, ...]}\n",
    "            ...}\n",
    "    Where \n",
    "    - `max` is the maximum frequency of usage over the entire time period \n",
    "    - `me(di)an_all` is the me(di)an of the frequencies of usage at all points in the time interval.\n",
    "    - `me(di)an_in_use` is the me(di)an of the frequencies of usage only when actually in use (frequency >0)  \n",
    "    \n",
    "    \n",
    "3. Concatenate the final dictionaries\n",
    "4. Save as a single JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available parts of speech:\n",
    "- _PRON_\tpronoun\n",
    "- _DET_\tdeterminer or article\n",
    "- _ADP_\tan adposition: either a preposition or a postposition\n",
    "- _CONJ_\tconjunction\n",
    "- _PRT_\tparticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the \\*\\_CLOSED_CLASSES.json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "#For the Google POS tagging\n",
    "underscore = re.compile('_{1}')\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(directory,file_path):\n",
    "    with open(directory+file_path,'r') as f:\n",
    "        ngrams = json.load(f)\n",
    "        f.close()\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ngrams):\n",
    "    years = [str(i) for i in range(1800,2020)]\n",
    "    unigram_dict = dict()\n",
    "    for word in tqdm(ngrams.keys()):\n",
    "        match_count_by_year = []\n",
    "        for year in years:\n",
    "            if year in ngrams[word].keys():\n",
    "                match_count_by_year.append(ngrams[word][year])\n",
    "            else:\n",
    "                #Zeroes are necessary for smoothing\n",
    "                match_count_by_year.append(0)\n",
    "        unigram_dict[word] = match_count_by_year\n",
    "    return unigram_dict, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(unigram_dict, years, smoothing = 5):\n",
    "    df = pd.DataFrame.from_dict(unigram_dict #take in the dictionary\n",
    "                    ).rolling(smoothing,center=True #create frames of size 5 (smoothing value), and replace value in middle\n",
    "                    ).mean( #average accross those frames\n",
    "                    ).rename({i:years[i] for i in range(len(years))}, axis = 'index' #rename the indices to years\n",
    "                    ).dropna()\n",
    "\n",
    "    years_map = {i:int(year) for i, year in enumerate(df.index)}\n",
    "    ngrams = df.to_dict(orient = 'list')\n",
    "    return ngrams, years_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_birth_and_death(ngrams,years_map):\n",
    "    ngrams_analyzed = {}\n",
    "    \n",
    "    for unigram in tqdm(ngrams.keys()):\n",
    "        frequency_list = ngrams[unigram]\n",
    "        frequency_in_use_list = [f for f in frequency_list if f>0]\n",
    "        if frequency_in_use_list: #only proceed if there is some value that is greater than 0 in the frequency list\n",
    "            max_usage = max(frequency_list)\n",
    "            median_all = statistics.median(frequency_list)\n",
    "            median_in_use = statistics.median(frequency_in_use_list)\n",
    "            mean_all = statistics.mean(frequency_list)\n",
    "            mean_in_use = statistics.mean(frequency_in_use_list)\n",
    "\n",
    "            birth_years, death_years = [],[]\n",
    "            for i in range(len(frequency_list)-1):\n",
    "                #Birth\n",
    "                if frequency_list[i]==0 and frequency_list[i+1]!=0:\n",
    "                    birth_years.append(years_map[i+1])\n",
    "                #Death\n",
    "                if frequency_list[i]!=0 and frequency_list[i+1]==0:\n",
    "                    death_years.append(years_map[i])\n",
    "                #Disregarding death in the final year\n",
    "\n",
    "            if len(birth_years)+len(death_years)>0:\n",
    "                #Replace the tagged unigram with the word and place POS separately\n",
    "                word_pos = underscore.split(unigram)\n",
    "                ngrams_analyzed[word_pos[0]] = {'POS':word_pos[1],\n",
    "                                                'max_usage':max_usage,\n",
    "                                                'median_all':median_all,\n",
    "                                                'median_in_use':median_in_use,\n",
    "                                                'mean_all':mean_all,\n",
    "                                                'mean_in_use':mean_in_use,\n",
    "                                                'birth_years':birth_years,\n",
    "                                                'death_years':death_years}\n",
    "        else:\n",
    "            pass\n",
    "            #print(unigram,'had no instances of usage after smoothing.')\n",
    "            \n",
    "    return ngrams_analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(dictionary,directory,file_path):\n",
    "    output = file_path+'.json'\n",
    "    if len(dictionary)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            json.dump(dictionary, f_out)\n",
    "        print('SAVED: ',output,len(dictionary))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened 1-00006-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16626/16626 [00:00<00:00, 19467.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16626/16626 [00:05<00:00, 3175.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00007-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9092/9092 [00:00<00:00, 18436.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9092/9092 [00:02<00:00, 3149.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00008-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2891/2891 [00:00<00:00, 19312.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2891/2891 [00:00<00:00, 3331.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00009-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4368/4368 [00:00<00:00, 21408.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4368/4368 [00:01<00:00, 3018.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00010-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12317/12317 [00:00<00:00, 22474.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12317/12317 [00:04<00:00, 3005.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00011-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4995/4995 [00:00<00:00, 17390.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4995/4995 [00:01<00:00, 3244.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00012-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 12206/12206 [00:00<00:00, 20733.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12206/12206 [00:03<00:00, 3198.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00013-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3739/3739 [00:00<00:00, 19022.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3739/3739 [00:01<00:00, 3165.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00014-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 15740/15740 [00:00<00:00, 21261.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 15740/15740 [00:05<00:00, 2901.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00015-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 13055/13055 [00:00<00:00, 21320.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13055/13055 [00:04<00:00, 3058.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00016-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 123093/123093 [00:06<00:00, 19678.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 123093/123093 [00:40<00:00, 3007.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00017-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20180/20180 [00:00<00:00, 21364.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20180/20180 [00:06<00:00, 2955.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00018-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 36568/36568 [00:01<00:00, 19040.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 36568/36568 [00:12<00:00, 2893.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00019-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 72093/72093 [00:03<00:00, 21245.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 72093/72093 [00:25<00:00, 2841.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00020-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 77955/77955 [00:04<00:00, 16587.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 77955/77955 [00:26<00:00, 2940.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00021-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 41723/41723 [00:02<00:00, 18436.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 41723/41723 [00:14<00:00, 2861.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00022-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 33454/33454 [00:01<00:00, 18652.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 33454/33454 [00:11<00:00, 2821.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "Opened 1-00023-of-00024_CLOSED_CLASSES.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 122872/122872 [00:06<00:00, 19442.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized\n",
      "Smoothed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 122872/122872 [00:43<00:00, 2822.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed birth and death\n",
      "SAVED:  CLOSED_CLASSES_SORTABLE.json 440403\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "\n",
    "directory = '../Ngrams/unigram_data/'\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file_path in files:\n",
    "    if '_CLOSED_CLASSES.json' in file_path:\n",
    "        ngrams = open_json(directory,file_path)\n",
    "        print('Opened',file_path)\n",
    "        unigram_dict, years = normalize(ngrams)\n",
    "        print('Normalized')\n",
    "        del ngrams\n",
    "        ngrams, years_map = smoothing(unigram_dict, years)\n",
    "        print('Smoothed')\n",
    "        del unigram_dict\n",
    "        del years\n",
    "        ngrams_analyzed = analyze_birth_and_death(ngrams,years_map)\n",
    "        print('Analyzed birth and death')\n",
    "        del ngrams\n",
    "        del years_map\n",
    "        final_dict.update(ngrams_analyzed)\n",
    "        del ngrams_analyzed\n",
    "save_json(final_dict,directory,'CLOSED_CLASSES_SORTABLE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evol-ling]",
   "language": "python",
   "name": "conda-env-evol-ling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
