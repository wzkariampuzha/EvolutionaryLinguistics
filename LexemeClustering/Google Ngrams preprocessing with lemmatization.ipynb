{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Ngrams Analysis\n",
    "## An Evolutionary Investigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original Ngrams analysis](https://github.com/Aaronasnx/Google-preprocessing/blob/main/ngram%20project.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install Unidecode\n",
    "from unidecode import unidecode\n",
    "#For the Google POS tagging mapping\n",
    "underscore = re.compile('_{1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NLTK POS Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "            `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "            for [satellite adjectives](https://stackoverflow.com/questions/18817396/what-part-of-speech-does-s-stand-for-in-wordnet-synsets).  \n",
    "  \n",
    "  Syntax:\n",
    "`lemmatizer.lemmatize(word)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google Tags](https://books.google.com/ngrams/info)\n",
    "These tags can either stand alone (\\_PRON\\_) or can be appended to a word (she_PRON)\n",
    "- _NOUN_\t\t\n",
    "- _VERB_\t\n",
    "- _ADJ_\tadjective\n",
    "- _ADV_\tadverb\n",
    "- _PRON_\tpronoun\n",
    "- _DET_\tdeterminer or article\n",
    "- _ADP_\tan adposition: either a preposition or a postposition\n",
    "- _NUM_\tnumeral\n",
    "- _CONJ_\tconjunction\n",
    "- _PRT_\tparticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation).union({'“','”'})\n",
    "#ALPHABET = set(string.ascii_letters)\n",
    "\n",
    "DIGITS = set(string.digits)\n",
    "VOWELS = set(\"aeiouyAEIOUY\")\n",
    "#Excluding '_' (underscore) from DASHES precludes the tagged 1grams \"_NOUN\", add it to also include the tagged 1grams\n",
    "DASHES = {'—','–','—','―','‒','-','_'}\n",
    "PUNCTUATION.difference_update(DASHES)\n",
    "STOPS = PUNCTUATION.union(DIGITS)\n",
    "#GOOGLE_TAGS = {'_NOUN','_VERB','_ADJ','_ADV','_PRON','_DET','_ADP','_NUM','_CONJ','_PRT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dias'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Demo of unidecode to show how will use it to filter out accents and non-English letters\n",
    "unidecode('días', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "unigram = 'kožušček'\n",
    "test = unidecode(unigram, errors='replace')\n",
    "if test == unigram:\n",
    "    print('yes')\n",
    "    pass\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to open Gzip files](https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_gzip(directory,file_path):\n",
    "    with gzip.open(directory+file_path,'r') as f_in:\n",
    "        rows = [x.decode('utf8').strip() for x in f_in.readlines()]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2tuple(string):\n",
    "    year,match_count,volume_count = tuple(string.split(','))\n",
    "    return int(year),int(match_count),int(volume_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(ngram_dict,directory,file_path):\n",
    "    output = file_path[:-3]+'-lemmatized.json'\n",
    "    if len(ngram_dict)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            json.dump(ngram_dict, f_out)\n",
    "        print('SAVED: ',output,len(ngram_dict))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tests(unigram):\n",
    "    #Checks each character in the unigram against the characters in the STOP set. (character level filtering) - no punctuation or digits allowed\n",
    "    if set(unigram).intersection(STOPS):\n",
    "        return False\n",
    "    \n",
    "    #Excluded all of the form _PRON_\n",
    "    if unigram[0] == '_' and unigram[-1] == '_':\n",
    "        return False\n",
    "    \n",
    "    #must have a vowel (presupposes that it must also have a letter of the alphabet inside)\n",
    "    if not set(unigram).intersection(VOWELS):\n",
    "        return False #Rewrite the alphabet one, i think this is better\n",
    "    \n",
    "    #If there are no letters of the alphabet inside\n",
    "    #if not set(unigram).intersection(ALPHABET):\n",
    "    #    return False\n",
    "    \n",
    "    #Words cannot start or end with dashes\n",
    "    if unigram[0] in DASHES or unigram[-1] in DASHES:\n",
    "        return False\n",
    "    \n",
    "    #Exclude words with more than one underscore, can make this != to only include tagged words\n",
    "    if len(underscore.findall(unigram))>1:\n",
    "        return False\n",
    "    \n",
    "    #must have 0 non-english letters\n",
    "    test = unidecode(unigram, errors='replace')\n",
    "    if test != unigram:\n",
    "        return False\n",
    "    \n",
    "    #Can implement more tests here if you need to do more filtering\n",
    "    \n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps Google pos_tag to Wordnet pos_tag\n",
    "def POS_mapper(pos_tag):\n",
    "    if pos_tag == 'NOUN':\n",
    "        return \"n\"\n",
    "    if pos_tag == 'VERB':\n",
    "        return \"v\"\n",
    "    if pos_tag == 'ADJ':\n",
    "        return \"a\"\n",
    "    if pos_tag == 'ADV':\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"n\" #Default for wordnet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ngrams(directory,file_path):\n",
    "    \n",
    "    rows = open_gzip(directory,file_path)\n",
    "    ngram_dict = dict()\n",
    "\n",
    "    #This implementation uses {1gram:{year:match_count ...} ...}\n",
    "    i=0\n",
    "    for row in rows:\n",
    "        columns = row.split('\\t')\n",
    "        #unigram is the first entry, the rest of the entries are of the form year,match_count,volume_count\\t n times, where n is variable each line\n",
    "        \n",
    "        unigram = columns[0]\n",
    "        #If it passes the word tests continue parsing and lemmatizing the unigram\n",
    "        if unigram_tests(unigram):\n",
    "            pos = \"n\" #Default for wordnet lemmatizer\n",
    "            if len(underscore.findall(unigram))==1: #One and only one underscore allowed\n",
    "                word_tag = unigram.split('_') # list of [word,tag]\n",
    "                #maps Google tag to Wordnet tag\n",
    "                pos = POS_mapper(word_tag[1])\n",
    "                #Removes the tag before processing unigram string\n",
    "                unigram = word_tag[0]\n",
    "            #can indent all after here if you only want tagged unigrams (easier than adding the condition to the unigram tests)\n",
    "            #Lemmatize based on POS\n",
    "            unigram = lemmatizer.lemmatize(unigram.lower().strip(),pos)\n",
    "            \n",
    "            #Parse the new entry and create a list of records in form [...[year, match_count]...]\n",
    "            records = dict()\n",
    "            for entry in columns[1:]:\n",
    "                year,match_count,volume_count = csv2tuple(str(entry))\n",
    "                if year>1800 and volume_count>1:\n",
    "                    records[year] = match_count\n",
    "\n",
    "            #Modify the dictionary if new entry is already there, else just add it as a new unigram:records to the dict\n",
    "            if unigram in ngram_dict.keys():\n",
    "                #accessing the ngram dictionary and seeing if each year is present, if so add match count, else add a new record entry to the dictionary.\n",
    "                for yr, match_ct in records.items(): #each record should be of the form {year, match_count}\n",
    "                    #If the year in the new record is in the dict for this 1gram, then find where it is.\n",
    "                    if yr in ngram_dict[unigram].keys():\n",
    "                        ngram_dict[unigram][yr] += match_ct\n",
    "                    else:\n",
    "                        #This just adds the record to the end, will need to sort later\n",
    "                        ngram_dict[unigram][yr] = match_ct\n",
    "            else:\n",
    "                ngram_dict[unigram] = records\n",
    "\n",
    "        i+=1\n",
    "        if i%500000==0:\n",
    "            print(i)\n",
    "    \n",
    "    #Save as JSON\n",
    "    save_json(ngram_dict,directory,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "unigram dict empty 1-00000-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00015-of-00024-lemmatized.json 971980\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00009-of-00024-lemmatized.json 999089\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00010-of-00024-lemmatized.json 1102950\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00022-of-00024-lemmatized.json 1114873\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00019-of-00024-lemmatized.json 1038908\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "unigram dict empty 1-00005-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00006-of-00024-lemmatized.json 526109\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00021-of-00024-lemmatized.json 977051\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00013-of-00024-lemmatized.json 1124605\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00016-of-00024-lemmatized.json 1108326\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "unigram dict empty 1-00003-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "unigram dict empty 1-00004-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00018-of-00024-lemmatized.json 1015168\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "SAVED:  1-00023-of-00024-lemmatized.json 1072972\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00011-of-00024-lemmatized.json 1122554\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00008-of-00024-lemmatized.json 1138422\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00014-of-00024-lemmatized.json 1254296\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "unigram dict empty 1-00001-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "unigram dict empty 1-00002-of-00024-lemmatized.json\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00017-of-00024-lemmatized.json 1085485\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00012-of-00024-lemmatized.json 990373\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00020-of-00024-lemmatized.json 1134392\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "SAVED:  1-00007-of-00024-lemmatized.json 1085482\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory = './Ngrams/'\n",
    "files = os.listdir(directory)\n",
    "for file_path in files:\n",
    "    if '.gz' in file_path:\n",
    "        preprocess_ngrams(directory,file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
