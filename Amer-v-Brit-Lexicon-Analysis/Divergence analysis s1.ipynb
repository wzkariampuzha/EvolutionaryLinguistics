{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence Analysis\n",
    "\n",
    "## First look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_lexicon import open_json\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from tqdm import tqdm\n",
    "#import os\n",
    "#import statistics\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amer_directory = '../Ngrams/amer_unigram_data/'\n",
    "brit_directory = '../Ngrams/brit_unigram_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1801-1850 Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIT_1801_1850_STEP1 = open_json(brit_directory,'LEXICON_1801-1850_STEP1.json')\n",
    "AMER_1801_1850_STEP1 = open_json(amer_directory,'LEXICON_1801-1850_STEP1.json')\n",
    "\n",
    "brit_1801_1850_s1 = set(BRIT_1801_1850_STEP1.keys())\n",
    "amer_1801_1850_s1 = set(AMER_1801_1850_STEP1.keys())\n",
    "\n",
    "intersection_1801_1850 = brit_1801_1850_s1.intersection(amer_1801_1850_s1)\n",
    "amer_unique_1801_1850_s1 = amer_1801_1850_s1.difference(intersection_1801_1850)\n",
    "brit_unique_1801_1850_s1 = brit_1801_1850_s1.difference(intersection_1801_1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106474, 37196, 37694)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brit_1801_1850_s1),len(intersection_1801_1850),len(amer_1801_1850_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 69278)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amer_unique_1801_1850_s1),len(brit_unique_1801_1850_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amer_unique_1801_1850_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brit_unique_1801_1850_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1851-1900 Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIT_1851_1900_STEP1 = open_json(brit_directory,'LEXICON_1851-1900_STEP1.json')\n",
    "AMER_1851_1900_STEP1 = open_json(amer_directory,'LEXICON_1851-1900_STEP1.json')\n",
    "\n",
    "brit_1851_1900_s1 = set(BRIT_1851_1900_STEP1.keys())\n",
    "amer_1851_1900_s1 = set(AMER_1851_1900_STEP1.keys())\n",
    "\n",
    "intersection_1851_1900 = brit_1851_1900_s1.intersection(amer_1851_1900_s1)\n",
    "amer_unique_1851_1900_s1 = amer_1851_1900_s1.difference(intersection_1851_1900)\n",
    "brit_unique_1851_1900_s1 = brit_1851_1900_s1.difference(intersection_1851_1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244666, 183790, 230103)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brit_1851_1900_s1),len(intersection_1851_1900),len(amer_1851_1900_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46313, 60876)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amer_unique_1851_1900_s1),len(brit_unique_1851_1900_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1901-1950 Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIT_1901_1950_STEP1 = open_json(brit_directory,'LEXICON_1901-1950_STEP1.json')\n",
    "AMER_1901_1950_STEP1 = open_json(amer_directory,'LEXICON_1901-1950_STEP1.json')\n",
    "\n",
    "brit_1901_1950_s1 = set(BRIT_1901_1950_STEP1.keys())\n",
    "amer_1901_1950_s1 = set(AMER_1901_1950_STEP1.keys())\n",
    "\n",
    "intersection_1901_1950 = brit_1901_1950_s1.intersection(amer_1901_1950_s1)\n",
    "amer_unique_1901_1950_s1 = amer_1901_1950_s1.difference(intersection_1901_1950)\n",
    "brit_unique_1901_1950_s1 = brit_1901_1950_s1.difference(intersection_1901_1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174324, 164436, 451000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brit_1901_1950_s1),len(intersection_1901_1950),len(amer_1901_1950_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286564, 9888)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amer_unique_1901_1950_s1),len(brit_unique_1901_1950_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amer_unique_1901_1950_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brit_unique_1901_1950_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1951-2000 Set Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIT_1951_2000_STEP1 = open_json(brit_directory,'LEXICON_1951-2000_STEP1.json')\n",
    "AMER_1951_2000_STEP1 = open_json(amer_directory,'LEXICON_1951-2000_STEP1.json')\n",
    "\n",
    "brit_1951_2000_s1 = set(BRIT_1951_2000_STEP1.keys())\n",
    "amer_1951_2000_s1 = set(AMER_1951_2000_STEP1.keys())\n",
    "\n",
    "intersection_1951_2000 = brit_1951_2000_s1.intersection(amer_1951_2000_s1)\n",
    "amer_unique_1951_2000_s1 = amer_1951_2000_s1.difference(intersection_1951_2000)\n",
    "brit_unique_1951_2000_s1 = brit_1951_2000_s1.difference(intersection_1951_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281984, 268910, 777075)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brit_1951_2000_s1),len(intersection_1951_2000),len(amer_1951_2000_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508165, 13074)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amer_unique_1951_2000_s1),len(brit_unique_1951_2000_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amer_unique_1951_2000_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brit_unique_1951_2000_s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "amer_unique = amer_unique_1951_2000_s1.intersection(amer_unique_1901_1950_s1,amer_unique_1851_1900_s1,amer_unique_1801_1850_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "brit_unique = brit_unique_1951_2000_s1.intersection(brit_unique_1901_1950_s1,brit_unique_1851_1900_s1,brit_unique_1801_1850_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 147)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amer_unique),len(brit_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abeel_NOUN',\n",
       " 'agawam_NOUN',\n",
       " 'agreea_NOUN',\n",
       " 'alatamaha_NOUN',\n",
       " 'amendatory_ADJ',\n",
       " 'amendatory_NOUN',\n",
       " 'bioren_NOUN',\n",
       " 'boudinot_NOUN',\n",
       " 'chenango_NOUN',\n",
       " 'chilicothe_NOUN',\n",
       " 'deposeth_NOUN',\n",
       " 'dutchess_ADJ',\n",
       " 'edenton_NOUN',\n",
       " 'ferriage_NOUN',\n",
       " 'governeur_NOUN',\n",
       " 'gpd_NOUN',\n",
       " 'harrisburgh_NOUN',\n",
       " 'haverstraw_NOUN',\n",
       " 'hopkinton_NOUN',\n",
       " 'hunterdon_NOUN',\n",
       " 'inhab_ADJ',\n",
       " 'kaskaskias_NOUN',\n",
       " 'kennebeck_NOUN',\n",
       " 'kennebunk_NOUN',\n",
       " 'ladelphia_NOUN',\n",
       " 'lansingburgh_NOUN',\n",
       " 'lewistown_NOUN',\n",
       " 'lumbia_NOUN',\n",
       " 'machias_NOUN',\n",
       " 'massac_NOUN',\n",
       " 'medfield_NOUN',\n",
       " 'mohegan_NOUN',\n",
       " 'natches_NOUN',\n",
       " 'necticut_NOUN',\n",
       " 'newhampshire_NOUN',\n",
       " 'newjersey_NOUN',\n",
       " 'newlondon_NOUN',\n",
       " 'neworleans_NOUN',\n",
       " 'northamerica_NOUN',\n",
       " 'northcarolina_NOUN',\n",
       " 'oconee_NOUN',\n",
       " 'olence_NOUN',\n",
       " 'onondago_NOUN',\n",
       " 'pascalis_NOUN',\n",
       " 'pittstown_NOUN',\n",
       " 'puking_NOUN',\n",
       " 'purviance_NOUN',\n",
       " 'rhodeisland_NOUN',\n",
       " 'rockbridge_NOUN',\n",
       " 'rolina_NOUN',\n",
       " 'sachusetts_NOUN',\n",
       " 'saluda_NOUN',\n",
       " 'schoharie_NOUN',\n",
       " 'segars_NOUN',\n",
       " 'shajl_VERB',\n",
       " 'shawanese_NOUN',\n",
       " 'simsbury_NOUN',\n",
       " 'sollowing_VERB',\n",
       " 'southcarolina_NOUN',\n",
       " 'stoddert_NOUN',\n",
       " 'stratham_NOUN',\n",
       " 'surther_VERB',\n",
       " 'swartwout_NOUN',\n",
       " 'tennesse_NOUN',\n",
       " 'ticut_NOUN',\n",
       " 'truxton_NOUN',\n",
       " 'varnum_NOUN',\n",
       " 'warrantee_NOUN',\n",
       " 'weathersfield_NOUN',\n",
       " 'whitestown_NOUN',\n",
       " 'yadkin_NOUN',\n",
       " 'yea_NUM'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amer_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aberlady_NOUN',\n",
       " 'accompte_NOUN',\n",
       " 'addingham_NOUN',\n",
       " 'alphage_NOUN',\n",
       " 'annuatim_X',\n",
       " 'aprilis_X',\n",
       " 'aucht_NOUN',\n",
       " 'avourable_ADJ',\n",
       " 'bassishaw_NOUN',\n",
       " 'bathwick_NOUN',\n",
       " 'befoir_ADP',\n",
       " 'begyn_VERB',\n",
       " 'benefice_ADP',\n",
       " 'boxhill_NOUN',\n",
       " 'burgi_X',\n",
       " 'bwlch_NOUN',\n",
       " 'cambuskenneth_NOUN',\n",
       " 'campvere_NOUN',\n",
       " 'cawsand_NOUN',\n",
       " 'cerrig_NOUN',\n",
       " 'chalder_NOUN',\n",
       " 'chapelries_NOUN',\n",
       " 'clipstone_NOUN',\n",
       " 'coldham_NOUN',\n",
       " 'colourmen_NOUN',\n",
       " 'contynue_VERB',\n",
       " 'courtown_NOUN',\n",
       " 'croftes_NOUN',\n",
       " 'dalswinton_NOUN',\n",
       " 'dapifer_NOUN',\n",
       " 'decern_VERB',\n",
       " 'drummore_NOUN',\n",
       " 'dunging_NOUN',\n",
       " 'dunstanville_NOUN',\n",
       " 'easingwold_NOUN',\n",
       " 'edwardi_X',\n",
       " 'eglwys_NOUN',\n",
       " 'exciseable_ADJ',\n",
       " 'favourites_VERB',\n",
       " 'feuars_NOUN',\n",
       " 'feued_VERB',\n",
       " 'fiars_NOUN',\n",
       " 'flixton_NOUN',\n",
       " 'fornham_NOUN',\n",
       " 'fourtie_NUM',\n",
       " 'galfrid_NOUN',\n",
       " 'glatton_NOUN',\n",
       " 'gwennap_NOUN',\n",
       " 'haughs_NOUN',\n",
       " 'hawise_NOUN',\n",
       " 'highworth_NOUN',\n",
       " 'hildersham_NOUN',\n",
       " 'hilsea_NOUN',\n",
       " 'hitcham_NOUN',\n",
       " 'holbourne_NOUN',\n",
       " 'humberston_NOUN',\n",
       " 'humbie_NOUN',\n",
       " 'iabour_NOUN',\n",
       " 'infeft_VERB',\n",
       " 'infeftment_NOUN',\n",
       " 'inspeximus_NOUN',\n",
       " 'kennoway_NOUN',\n",
       " 'kilmaine_NOUN',\n",
       " 'kinfauns_NOUN',\n",
       " 'kircudbright_NOUN',\n",
       " 'kirkmichael_NOUN',\n",
       " 'lanthony_NOUN',\n",
       " 'lexden_NOUN',\n",
       " 'llandilo_NOUN',\n",
       " 'madocks_NOUN',\n",
       " 'mapletoft_NOUN',\n",
       " 'measham_NOUN',\n",
       " 'meigle_NOUN',\n",
       " 'merks_VERB',\n",
       " 'methwold_NOUN',\n",
       " 'micklegate_NOUN',\n",
       " 'modbury_NOUN',\n",
       " 'monopolising_NOUN',\n",
       " 'mornynge_NOUN',\n",
       " 'multure_NOUN',\n",
       " 'murdac_NOUN',\n",
       " 'nelthorpe_NOUN',\n",
       " 'nemorum_NOUN',\n",
       " 'netherbow_NOUN',\n",
       " 'newbottle_NOUN',\n",
       " 'newenden_NOUN',\n",
       " 'northwold_NOUN',\n",
       " 'ochtertyre_NOUN',\n",
       " 'ombersley_NOUN',\n",
       " 'oxendon_NOUN',\n",
       " 'pairt_VERB',\n",
       " 'parliamentum_NOUN',\n",
       " 'paroch_NOUN',\n",
       " 'patrington_NOUN',\n",
       " 'pechell_NOUN',\n",
       " 'pemb_NOUN',\n",
       " 'pencaitland_NOUN',\n",
       " 'penmaen_NOUN',\n",
       " 'pittenweem_NOUN',\n",
       " 'portioner_NOUN',\n",
       " 'portioners_NOUN',\n",
       " 'portsoy_NOUN',\n",
       " 'prebendal_NOUN',\n",
       " 'presentlie_NOUN',\n",
       " 'presteign_NOUN',\n",
       " 'protulit_X',\n",
       " 'provyde_VERB',\n",
       " 'quhen_ADV',\n",
       " 'quhen_X',\n",
       " 'quhilk_VERB',\n",
       " 'repayred_VERB',\n",
       " 'ridware_NOUN',\n",
       " 'righthon_NOUN',\n",
       " 'sedgmoor_NOUN',\n",
       " 'shacklewell_NOUN',\n",
       " 'soche_ADJ',\n",
       " 'southover_NOUN',\n",
       " 'speciallie_NOUN',\n",
       " 'spithead_VERB',\n",
       " 'splendour_ADV',\n",
       " 'stretham_NOUN',\n",
       " 'teinds_NOUN',\n",
       " 'thame_PRON',\n",
       " 'thame_VERB',\n",
       " 'thatcham_NOUN',\n",
       " 'thornham_NOUN',\n",
       " 'thornhaugh_NOUN',\n",
       " 'thyngs_NOUN',\n",
       " 'tillicoultry_NOUN',\n",
       " 'tinwald_NOUN',\n",
       " 'tokenhouse_NOUN',\n",
       " 'traeth_NOUN',\n",
       " 'ulbster_NOUN',\n",
       " 'unfavour_ADJ',\n",
       " 'uponthames_NOUN',\n",
       " 'upontweed_NOUN',\n",
       " 'upwell_NOUN',\n",
       " 'warblington_NOUN',\n",
       " 'warnford_NOUN',\n",
       " 'whittlebury_NOUN',\n",
       " 'widmore_NOUN',\n",
       " 'wilsden_NOUN',\n",
       " 'winchelsey_NOUN',\n",
       " 'wingerworth_NOUN',\n",
       " 'wolsingham_NOUN',\n",
       " 'woodchurch_NOUN',\n",
       " 'woolled_VERB'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brit_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Divergence\n",
    "As the datasets are extremely imbalanced in terms of size, we will only investigate the differences between the 1851-1900 sets as they are the most closely balanced.\n",
    "\n",
    "We will \n",
    "1. calculate Jaccard similarity index\n",
    "2. calculate Wright's Fixation Index (a measure of how different the alleles in each population are)\n",
    "3. plot the Zipfian distribution of both of these sets\n",
    "4. take a qualitative look into the similarities and differences between the British and American English words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jaccard Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard similarity of the British and American lexicons in 1851-1900 is 63.16 %\n"
     ]
    }
   ],
   "source": [
    "jaccard = len(intersection_1851_1900)/len(brit_1851_1900_s1.union(amer_1851_1900_s1))\n",
    "print('The Jaccard similarity of the British and American lexicons in 1851-1900 is', \"{:.2f}\".format(jaccard*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sample-Size Adjusted $X^2$ Test\n",
    "We will consider consider British English to be the parent language (containing the expected distribution) under the null hypothesis that there has been no divergence. Thus,\n",
    "$A_i$ = frequency of the American lexeme\n",
    "$B_i$ = frequency of British lexeme.\n",
    "<br>\n",
    "<br>\n",
    "$$\\chi^2 = \\sum \\frac{(A_i \\cdot \\frac{\\sum B_i}{\\sum A_i} -B_i )^2}{B_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get $\\sum B_i$ and $\\sum A_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71349102873 49614962348 0.6953831281707026\n"
     ]
    }
   ],
   "source": [
    "B_sum = sum([BRIT_1851_1900_STEP1[lexeme]['sum_usage'] for lexeme in BRIT_1851_1900_STEP1.keys()])\n",
    "A_sum = sum([AMER_1851_1900_STEP1[lexeme]['sum_usage'] for lexeme in AMER_1851_1900_STEP1.keys()])\n",
    "R = B_sum/A_sum\n",
    "print(A_sum, B_sum, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121383666299.24712 183789 0.0\n"
     ]
    }
   ],
   "source": [
    "chisqd = sum([(AMER_1851_1900_STEP1[word]['sum_usage']*R\n",
    "              -BRIT_1851_1900_STEP1[word]['sum_usage']\n",
    "             )**2 / BRIT_1851_1900_STEP1[word]['sum_usage']\n",
    "             for word in intersection_1851_1900])\n",
    "\n",
    "dfreedom = len(intersection_1851_1900) - 1\n",
    "\n",
    "from scipy.stats import chi2\n",
    "p = 1 - chi2.cdf(chisqd, dfreedom)\n",
    "\n",
    "print(chisqd, dfreedom, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49475884925 49458166253 17718672\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "from math import floor\n",
    "observed = [floor(AMER_1851_1900_STEP1[word]['sum_usage']*R) for word in intersection_1851_1900]\n",
    "expected = [BRIT_1851_1900_STEP1[word]['sum_usage'] for word in intersection_1851_1900]\n",
    "chisqd, p = chisquare(observed, expected)\n",
    "print(chisqd, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_obs = sum(observed) \n",
    "sum_exp = sum(expected)\n",
    "if not sum_obs == sum_exp:\n",
    "    print(sum_obs, sum_exp, sum_obs-sum_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!\n"
     ]
    }
   ],
   "source": [
    "fix = sum_obs-sum_exp\n",
    "while fix>0:\n",
    "    for i, obs in enumerate(observed):\n",
    "        observed[i] = obs-1\n",
    "        fix-=1\n",
    "        if fix<1:\n",
    "            break\n",
    "\n",
    "sum_obs = sum(observed) \n",
    "sum_exp = sum(expected)\n",
    "if not sum_obs == sum_exp:\n",
    "    print(sum_obs, sum_exp, sum_obs-sum_exp)\n",
    "else:\n",
    "    print('Yay!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121363940942.0272 0.0\n"
     ]
    }
   ],
   "source": [
    "chisqd, p = chisquare(observed, expected)\n",
    "print(chisqd, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "183789\n"
     ]
    }
   ],
   "source": [
    "print(len(observed)==len(expected))\n",
    "df = len(observed)-1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalized and Sample-Size Adjusted $X^2$ Test\n",
    "We will consider consider British English to be the parent language (containing the expected distribution) under the null hypothesis that there has been no divergence. Thus,\n",
    "$A_i$ = frequency of the American lexeme\n",
    "$B_i$ = frequency of British lexeme.\n",
    "<br>\n",
    "<br>\n",
    "Based on the [derivation](https://github.com/wzkariampuzha/EvolutionaryLinguistics/tree/main/Amer-v-Brit-Lexicon-Analysis#Derivation-of-Normalized-and-Sample-Size-Adjusted-Test)\n",
    "$$\\chi^2 = \\sum B_i \\cdot \\sum \\frac{(\\frac{A_i}{\\sum{A_i}} -\\frac{B_i}{\\sum{B_i}})^2}{\\frac{B_i}{\\sum{B_i}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqd_diffs = [(BRIT_1851_1900_STEP1[word]['sum_usage']-AMER_1851_1900_STEP1[word]['frequency'])**2 for word in intersection_1851_1900]\n",
    "#These two would be subtracting by zero frequency from other set\n",
    "sqd_diffs += [BRIT_1851_1900_STEP1[word]['frequency']**2 for word in brit_unique_1851_1900_s1]\n",
    "sqd_diffs += [AMER_1851_1900_STEP1[word]['frequency']**2 for word in amer_unique_1851_1900_s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoms = [BRIT_1851_1900_STEP1[word]['sum_usage'] for word in intersection_1851_1900]\n",
    "denoms+= [BRIT_1851_1900_STEP1[word]['frequency'] for word in brit_unique_1851_1900_s1]\n",
    "denoms+= [0 for word in amer_unique_1851_1900_s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sqd_diffs)==len(denoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_sum = sum([BRIT_1851_1900_STEP1[lexeme]['sum_usage'] for lexeme in BRIT_1851_1900_STEP1.keys()])\n",
    "B_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisqd = B_sum * sum([diff/denom for diff,denom in zip(sqd_diffs, denoms)])\n",
    "dfreedom = len(sqd_diffs) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get p-value\n",
    "<br>\n",
    "[Citation](https://stackoverflow.com/questions/11725115/p-value-from-chi-sq-test-statistic-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "p = 1 - chi2.cdf(chisqd, dfreedom)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wright's Fixation Index $F_{ST}$\n",
    "\n",
    "Wright's Fixation Index is a measure of how different the alleles in each population are.\n",
    "\n",
    "$$F_{ST} = \\frac{H_T-H_S}{H_T}$$\n",
    "\n",
    "Where heterozygosity $H$ is measured as \n",
    "$$1-\\sum(f^2)$$\n",
    "\n",
    "$H_T$ is the heterozygosity of the entire population. $H_S$ is the weighted heterozygosity of each individual population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pairwise Frequency-Dependent Similarity\n",
    "\n",
    "$$1-\\sum(f_1-f_2)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqd_diff = sum([(BRIT_1851_1900_STEP1[word]['frequency']-AMER_1851_1900_STEP1[word]['frequency'])**2 for word in intersection_1851_1900])\n",
    "#These two would be subtracting by zero frequency from other set\n",
    "sqd_diff += sum((BRIT_1851_1900_STEP1[word]['frequency'])**2 for word in brit_unique_1851_1900_s1)\n",
    "sqd_diff += sum((AMER_1851_1900_STEP1[word]['frequency'])**2 for word in amer_unique_1851_1900_s1)\n",
    "print(1-sqd_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sqd = sum([(AMER_1851_1900_STEP1[word]['frequency'] if word in AMER_1851_1900_STEP1.keys() else 0 \n",
    "               - \n",
    "               BRIT_1851_1900_STEP1[word]['frequency'] if word in BRIT_1851_1900_STEP1.keys() else 0)**2 \n",
    "               \n",
    "               for word in set(BRIT_1851_1900_STEP1.keys()).union(set(AMER_1851_1900_STEP1.keys()))\n",
    "              / \n",
    "              BRIT_1851_1900_STEP1[word]['frequency'] if word in BRIT_1851_1900_STEP1.keys() else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in set(BRIT_1851_1900_STEP1.keys()).union(set(AMER_1851_1900_STEP1.keys())):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heterozygosity(freq_iter):\n",
    "    return 1 - sum([f^2 for f in freq_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amer_unique_1851_1900_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brit_unique_1851_1900_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_counts(dictionary, count_type, num_hits=500, head = True):\n",
    "    return OrderedDict(sorted(dictionary.items(), key=lambda x: x[1][count_type], reverse=head)[:num_hits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amer_uniq_median_usage = {word:AMER_1851_1900_STEP1[word]['median_usage'] for word in amer_unique_1851_1900_s1}\n",
    "#amer_uniq_median_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brit_uniq_median_usage = {word:BRIT_1851_1900_STEP1[word]['median_usage'] for word in brit_unique_1851_1900_s1}\n",
    "#brit_uniq_median_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BRIT_1851_1900_STEP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_max_usage = top_counts(ngrams,'max_usage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evol-ling]",
   "language": "python",
   "name": "conda-env-evol-ling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
