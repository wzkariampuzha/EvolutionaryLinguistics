{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Ngrams Lexicon Preprocessing\n",
    "The purpose of this is to preprocess and normalize an entire Google Ngrams \\*.gz file dataset once so that subsequent analyses can be done quickly. This needs to be optimized with [`pandas`](https://pandas.pydata.org/) and [`numpy`](https://numpy.org/) to be faster and reduced memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous preprocessing notebook](https://github.com/wzkariampuzha/EvolutionaryLinguistics/blob/main/LexiconSize/Create%20English%20Language%20Complete%20Set%20(preprocessing%20with%20lemmatization).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "#for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#For checking if the word has any non-English-alphabetical letters\n",
    "from unidecode import unidecode\n",
    "\n",
    "import re\n",
    "#For the Google POS tagging mapping\n",
    "underscore = re.compile('_{1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NLTK POS Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "            `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "            for [satellite adjectives](https://stackoverflow.com/questions/18817396/what-part-of-speech-does-s-stand-for-in-wordnet-synsets).  \n",
    "  \n",
    "  Syntax:\n",
    "`lemmatizer.lemmatize(word)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google Tags](https://books.google.com/ngrams/info)\n",
    "These tags can either stand alone (\\_PRON\\_) or can be appended to a word (she_PRON)\n",
    "- _NOUN_\t\t\n",
    "- _VERB_\t\n",
    "- _ADJ_\tadjective\n",
    "- _ADV_\tadverb\n",
    "- _PRON_\tpronoun\n",
    "- _DET_\tdeterminer or article\n",
    "- _ADP_\tan adposition: either a preposition or a postposition\n",
    "- _NUM_\tnumeral\n",
    "- _CONJ_\tconjunction\n",
    "- _PRT_\tparticle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sets which are going to be used in the unigram tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation).union({'“','”'})\n",
    "#ALPHABET = set(string.ascii_letters)\n",
    "\n",
    "DIGITS = set(string.digits)\n",
    "VOWELS = set(\"aeiouyAEIOUY\")\n",
    "#Excluding '_' (underscore) from DASHES precludes the tagged 1grams \"_NOUN\", add it to also include the tagged 1grams\n",
    "DASHES = {'—','–','—','―','‒','-','_'}\n",
    "PUNCTUATION.difference_update(DASHES)\n",
    "STOPS = PUNCTUATION.union(DIGITS)\n",
    "#GOOGLE_TAGS = {'_NOUN','_VERB','_ADJ','_ADV','_PRON','_DET','_ADP','_NUM','_CONJ','_PRT'}\n",
    "\n",
    "#maps Google pos_tag to Wordnet pos_tag\n",
    "POS_mapper = {'NOUN':'n',\n",
    "              'VERB':'v',\n",
    "              'ADJ':'a',\n",
    "              'ADV':'v'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demo of unidecode to show how will use it to filter out accents and non-English letters\n",
    "unidecode('días', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = 'kožušček'\n",
    "test = unidecode(unigram, errors='replace')\n",
    "if test == unigram:\n",
    "    print('yes')\n",
    "    pass\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to open Gzip files](https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_gzip(directory,file_path):\n",
    "    with gzip.open(directory+file_path,'r') as f_in:\n",
    "        for line in f_in:\n",
    "            yield line.decode('utf8').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(ngram_dict,directory,file_path):\n",
    "    \n",
    "    output = file_path[:-3]+'-preprocessed.pickle'\n",
    "    if len(ngram_dict)>0:\n",
    "        with open(directory+output, 'w') as f_out:\n",
    "            pickle.dump(ngram_dict, f_out)\n",
    "        print('SAVED: ',output,len(ngram_dict))\n",
    "    else:\n",
    "        print('unigram dict empty',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Ngrams/unigram_data/ 1-00000-of-00024.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,387,805_NUM\n",
      "Wall time: 5 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory = '../Ngrams/unigram_data/'\n",
    "files = os.listdir(directory)\n",
    "for file_path in files:\n",
    "    if '.gz' in file_path and not '.json' in file_path:\n",
    "        print(directory,file_path)\n",
    "        for row in tqdm(open_gzip(directory,file_path)):\n",
    "            columns = row.split('\\t')\n",
    "            #unigram is the first entry, the rest of the entries are of the form year,match_count,volume_count\\t n times, where n is variable each line\n",
    "\n",
    "            unigram = columns[0]\n",
    "            print(unigram)\n",
    "            break\n",
    "        break\n",
    "        #preprocess_ngrams(directory,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2tuple(string):\n",
    "    year,match_count,volume_count = tuple(string.split(','))\n",
    "    return int(year),int(match_count),int(volume_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tests(unigram):\n",
    "    #Exclude words with more than one underscore, can make this != to only include tagged words\n",
    "    if len(underscore.findall(unigram))!=1:\n",
    "        return False\n",
    "    \n",
    "    #Checks each character in the unigram against the characters in the STOP set. (character level filtering) - no punctuation or digits allowed\n",
    "    if set(unigram).intersection(STOPS):\n",
    "        return False\n",
    "    \n",
    "    #Excluded all of the form _PRON_ (or anything that starts or ends with an underscore)\n",
    "    if unigram[0] == '_' or unigram[-1] == '_':\n",
    "        return False\n",
    "    \n",
    "    #must have a vowel (presupposes that it must also have a letter of the alphabet inside)\n",
    "    if not set(unigram).intersection(VOWELS):\n",
    "        return False \n",
    "    \n",
    "    #Words cannot start or end with dashes\n",
    "    if unigram[0] in DASHES or unigram[-1] in DASHES:\n",
    "        return False\n",
    "    \n",
    "    #must have 0 non-english letters\n",
    "    test = unidecode(unigram, errors='replace')\n",
    "    if test != unigram:\n",
    "        return False\n",
    "    \n",
    "    #Can implement more tests here if you need to do more filtering\n",
    "    \n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ngrams(directory,file_path):\n",
    "    \n",
    "    #rows = open_gzip(directory,file_path)\n",
    "    ngram_dict = dict()\n",
    "\n",
    "    #This implementation uses {1gram:{year:match_count ...} ...}\n",
    "    i=0\n",
    "    for row in tqdm(open_gzip(directory,file_path)):\n",
    "        columns = row.split('\\t')\n",
    "        #unigram is the first entry, the rest of the entries are of the form year,match_count,volume_count\\t n times, where n is variable each line\n",
    "        \n",
    "        unigram = columns[0]\n",
    "        #If it passes the word tests continue parsing and lemmatizing the unigram\n",
    "        if unigram_tests(unigram):\n",
    "            word_tag = underscore.split(unigram) # list of [word,tag]\n",
    "            \n",
    "            pos = \"n\" #Default for wordnet lemmatizer\n",
    "            if word_tag[1] in POS_mapper.keys():\n",
    "                pos = POS_mapper[word_tag[1]]\n",
    "            \n",
    "            #word_tag[0] removes the tag before processing unigram string\n",
    "            #Lemmatize based on POS\n",
    "            unigram = lemmatizer.lemmatize(word_tag[0].lower().strip(),pos)\n",
    "            \n",
    "            #Adds the tag back onto the unigram\n",
    "            unigram+='_'+word_tag[1]\n",
    "            \n",
    "            #Parse the new entry and create a dictionary of records in form {year:match_count}\n",
    "            records = dict()\n",
    "            for entry in columns[1:]:\n",
    "                year,match_count,volume_count = csv2tuple(str(entry))\n",
    "                #This is the crucial filtering by volume count because only words in >1 volume are reasonably assumed to be used by >1 person\n",
    "                #Words only used by one person - which translates the computational parameter 1 volume - are not considered part of the lexicon\n",
    "                if volume_count>1:\n",
    "                    records[year] = match_count\n",
    "\n",
    "            #Modify the dictionary if new entry is already there, else just add it as a new unigram:records to the dict\n",
    "            if unigram in ngram_dict.keys():\n",
    "                #accessing the ngram dictionary and seeing if each year is present, if so add match count, else add a new record entry to the dictionary.\n",
    "                for yr, match_ct in records.items(): #each record should be of the form {year:match_count}\n",
    "                    #If the year in the new record is in the dict for this 1gram, then find where it is.\n",
    "                    if yr in ngram_dict[unigram].keys():\n",
    "                        ngram_dict[unigram][yr] += match_ct\n",
    "                    else:\n",
    "                        #This just adds the record to the end, will need to sort later\n",
    "                        ngram_dict[unigram][yr] = match_ct\n",
    "            else:\n",
    "                ngram_dict[unigram] = records\n",
    "    #Save as JSON\n",
    "    save_json(ngram_dict,directory,file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evol-ling]",
   "language": "python",
   "name": "conda-env-evol-ling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
